{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct  9 16:28:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   37C    P8    26W / 300W |     28MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8     4W / 300W |     14MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       964      G   /usr/lib/xorg/Xorg                 15MiB |\n",
      "|    0   N/A  N/A      1181      G   /usr/bin/gnome-shell                8MiB |\n",
      "|    1   N/A  N/A       964      G   /usr/lib/xorg/Xorg                 10MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "title": "In [1]:"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/standardai/conformer/4422c3ea15024224a5d50ead81993c9f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "\n",
    "\n",
    "LOG_COMMET = True\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    import matplotlib.pyplot as plt\n",
    "    get_ipython().magic('matplotlib inline')\n",
    "    FIG_PATH = \"figures_{}\".format(datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\"))\n",
    "    os.mkdir(FIG_PATH)\n",
    "\n",
    "if LOG_COMMET:\n",
    "    experiment = Experiment(\n",
    "        api_key=\"7HshjkeTgLasPcZqrDwsPqq3J\",\n",
    "        project_name=\"conformer\",\n",
    "        workspace=\"standardai\",\n",
    "        auto_metric_logging=False, \n",
    "        log_code=True,\n",
    "    )\n",
    "        \n",
    "    experiment.set_name(\"mavi-audio-conformer #{}\".format(datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")))    \n",
    "    experiment.set_code()\n",
    "else:\n",
    "    experiment = Experiment(api_key=\"7HshjkeTgLasPcZqrDwsPqq3J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 2023\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.colorjitter import StaticColorJitter\n",
    "from utils.params import *\n",
    "from utils.data_prep import get_train_val_test\n",
    "from utils.dataset import DatasetFD, DATAMODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "model_path = \"mavi-audio-conformer_tolga.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "title": "In [3]:"
   },
   "outputs": [],
   "source": [
    "if LOG_COMMET:\n",
    "    experiment.log_parameters(param_dict)\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### DATA ###\n",
    "\n",
    "# Traverse Data from scratch\n",
    "df, train_idx, val_idx, test_idx = get_train_val_test()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "\n",
    "train_dataset = DatasetFD(df, train_idx, param_dict[\"n_sample_frames\"], param_dict[\"sampling_mode\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,num_workers=8, pin_memory=True, worker_init_fn=seed_worker)\n",
    "\n",
    "val_dataset = DatasetFD(df, val_idx, param_dict[\"n_sample_frames\"], param_dict[\"sampling_mode\"], istest=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False,num_workers=8, pin_memory=True, worker_init_fn=seed_worker)\n",
    "\n",
    "test_dataset = DatasetFD(df, test_idx, param_dict[\"n_sample_frames\"], param_dict[\"sampling_mode\"], istest=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=16, pin_memory=True, worker_init_fn=seed_worker)\n",
    "\n",
    "if LOG_COMMET:\n",
    "    experiment.log_parameters({\"num_videos\": len(df),\n",
    "                               \"train_size\": len(train_dataset),\n",
    "                               \"val_size\": len(val_dataset),\n",
    "                               \"test_size\": len(test_dataset),\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionAISummer(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=None):\n",
    "        \"\"\"\n",
    "        Implementation of multi-head attention layer of the original transformer model.\n",
    "        einsum and einops.rearrange is used whenever possible\n",
    "        Args:\n",
    "            dim: token's dimension, i.e. word embedding vector size\n",
    "            heads: the number of distinct representations to learn\n",
    "            dim_head: the dim of the head. In general dim_head<dim.\n",
    "            However, it may not necessary be (dim/heads)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        _dim = self.dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.to_qvk = nn.Linear(dim, _dim * 3, bias=False)\n",
    "        self.W_0 = nn.Linear( _dim, dim, bias=False)\n",
    "        self.scale_factor = self.dim_head ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        assert x.dim() == 3\n",
    "        # Step 1\n",
    "        qkv = self.to_qvk(x)  # [batch, tokens, dim*3*heads ]\n",
    "\n",
    "        # Step 2\n",
    "        # decomposition to q,v,k and cast to tuple\n",
    "        # the resulted shape before casting to tuple will be:\n",
    "        # [3, batch, heads, tokens, dim_head]\n",
    "        q, k, v = tuple(rearrange(qkv, 'b t (d k h) -> k b h t d ', k=3, h=self.heads))\n",
    "\n",
    "        # Step 3\n",
    "        # resulted shape will be: [batch, heads, tokens, tokens]\n",
    "        scaled_dot_prod = torch.einsum('b h i d , b h j d -> b h i j', q, k) * self.scale_factor\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.shape == scaled_dot_prod.shape[2:]\n",
    "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(scaled_dot_prod, dim=-1)\n",
    "\n",
    "        # Step 4. Calc result per batch and per head h\n",
    "        out = torch.einsum('b h i j , b h j d -> b h i d', attention, v)\n",
    "\n",
    "        # Step 5. Re-compose: merge heads with dim_head d\n",
    "        out = rearrange(out, \"b h t d -> b t (h d)\")\n",
    "\n",
    "        # Step 6. Apply final linear transformation layer\n",
    "        return self.W_0(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conformer = conformer = models.Conformer(\n",
    "                        input_dim=20,\n",
    "                        num_heads=4,\n",
    "                        ffn_dim=128,\n",
    "                        num_layers=4,\n",
    "                        depthwise_conv_kernel_size=31)\n",
    "        \n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,5], stride=[1,5])\n",
    "        self.lin = nn.Linear(14000, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):    \n",
    "        bs = x.shape[0]\n",
    "        x = x.permute(0,2,1) # batchx3500x20 -> batchx20x3500        \n",
    "        x = self.transformer_maxpool(x) # batchx20x3500 -> batchx20x700        \n",
    "        x = x.permute(0,2,1)        \n",
    "        x = self.conformer(x, torch.ones(bs).to(device)*700 ) # 700xbatchx20\n",
    "        x = x[0]        \n",
    "        #x = x.permute(1,0,2) # batchx20x700 -> 700xbatchx20\n",
    "        #x = torch.mean(x, dim=0) # batchx20\n",
    "        x = x.reshape(bs,-1) # batchx20x700 -> batchx14000\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "title": "In [8]:"
   },
   "outputs": [],
   "source": [
    "class CnnLstm(nn.Module):\n",
    "    def __init__(self, m_rgb_, m_d_):\n",
    "        super(CnnLstm, self).__init__()  \n",
    "        self.m_audio = AudioCNN()             \n",
    "    \n",
    "    def forward(self, x_rgb, x_d, x_a):\n",
    "        return self.m_audio(x_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "title": "In [10]:"
   },
   "outputs": [],
   "source": [
    "def plot_data(uid, frame_idx, imgs, labels, save_dict):\n",
    "    print(\"----\"*15)\n",
    "    print(df[[\"action\", \"bag_no\", \"bag_label\"]].iloc[uid.data.numpy()])\n",
    "\n",
    "    n_batch, n_frames, n_ch, h, w = imgs.size()\n",
    "\n",
    "    f, axarr = plt.subplots(n_batch,n_frames, figsize=(12,8))\n",
    "    for i in range(n_batch):\n",
    "        for j in range(n_frames):\n",
    "            im_ = imgs[i][j].numpy().transpose((1,2,0))            \n",
    "            axarr[i,j].imshow(im_, interpolation='nearest')\n",
    "            \n",
    "    fname = os.path.join(FIG_PATH, \"{}_{}_{}.png\".format(save_dict[\"name\"], save_dict[\"epoch\"], save_dict[\"batch_id\"]))\n",
    "    \n",
    "    plt.savefig(fname)\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "title": "In [11]:"
   },
   "outputs": [],
   "source": [
    "def test_epoch(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    all_labels, all_preds = [], []     \n",
    "    \n",
    "    batch_id = 0 \n",
    "    with experiment.test():\n",
    "        \n",
    "        for idx, frame_idx, imgs, dimgs, audio, labels in test_loader:\n",
    "            batch_id += 1\n",
    "            \n",
    "            if DEBUG and epoch % 10 == 0:\n",
    "                print(\"XX\"*20)                \n",
    "                print(\"Test Epoch:{}, Batch:{}\".format(epoch, batch_id))\n",
    "                if epoch == 0:\n",
    "                    plot_data(idx, frame_idx, imgs, labels, {\"epoch\": epoch, \"batch_id\": batch_id, \"name\": \"test\"})\n",
    "                \n",
    "            all_labels = np.concatenate((all_labels, labels.cpu().data.numpy()), axis=0)\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            dimgs= dimgs.to(device)\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)  \n",
    "            \n",
    "            if DATAMODE[\"RGBONLY\"]:                \n",
    "                output= model(imgs)\n",
    "            \n",
    "            elif DATAMODE[\"RGBD\"]:                \n",
    "                output= model(imgs, dimgs)\n",
    "                \n",
    "            elif DATAMODE[\"DONLY\"]:                \n",
    "                output= model(dimgs)\n",
    "                \n",
    "            elif DATAMODE[\"AONLY\"]:                \n",
    "                output= model(None, None, audio)\n",
    "            \n",
    "            elif DATAMODE[\"RGBDA\"]:                \n",
    "                output= model(imgs, dimgs, audio)\n",
    "                \n",
    "            loss = F.cross_entropy(output, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            values, indices = torch.max(torch.softmax(output, dim=1), 1)   \n",
    "\n",
    "            all_preds = np.concatenate((all_preds, indices.cpu().data.numpy()), axis=0)\n",
    "            \n",
    "            if DEBUG and epoch % 10 == 0:\n",
    "                print(\"Test preds: \", indices.cpu().data.numpy())\n",
    "            \n",
    "\n",
    "        test_loss = test_loss / len(test_dataset)        \n",
    "        test_acc = f1_score(all_labels, all_preds,average='weighted')        \n",
    "        \n",
    "        conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "        clf_report = classification_report(all_labels, all_preds)\n",
    "        \n",
    "        global best_acc\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            #torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                train_state = {                \n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.module.state_dict(),                \n",
    "                    'model': model,\n",
    "                }\n",
    "            else:\n",
    "                train_state = {                \n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),                \n",
    "                    'model': model,\n",
    "                }\n",
    "                \n",
    "            torch.save(\n",
    "                train_state,\n",
    "                model_path\n",
    "            )    \n",
    "            \n",
    "\n",
    "        if LOG_COMMET:\n",
    "            experiment.log_metric(\"f1_score\", test_acc, step=epoch)\n",
    "            experiment.log_metric(\"loss\", test_loss, step=epoch)        \n",
    "            experiment.log_confusion_matrix(matrix = conf_mat, step=epoch, file_name=\"conf_mat_test_{}.json\".format(epoch))        \n",
    "            experiment.log_text(clf_report, step=epoch)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "title": "In [12]:"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, epoch): \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    all_labels, all_preds = [], []     \n",
    "    \n",
    "    batch_id = 0\n",
    "    \n",
    "    with experiment.train():\n",
    "        \n",
    "        for idx, frame_idx, imgs, dimgs, audio, labels in train_loader:          \n",
    "            batch_id += 1            \n",
    "            \n",
    "            if DEBUG and epoch % 10 == 0:\n",
    "                print(\"//\"*15)\n",
    "                print(\"Train Epoch:{}, Batch:{}\".format(epoch, batch_id))\n",
    "                if epoch == 0:\n",
    "                    plot_data(idx, frame_idx, imgs, labels, {\"epoch\": epoch, \"batch_id\": batch_id, \"name\": \"train\"})\n",
    "                \n",
    "            imgs = imgs.to(device)\n",
    "            dimgs= dimgs.to(device)\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            all_labels = np.concatenate((all_labels, labels.cpu().data.numpy()), axis=0)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            if DATAMODE[\"RGBONLY\"]:                \n",
    "                output= model(imgs)\n",
    "            \n",
    "            elif DATAMODE[\"RGBD\"]:                \n",
    "                output= model(imgs, dimgs)\n",
    "                \n",
    "            elif DATAMODE[\"DONLY\"]:                \n",
    "                output= model(dimgs)\n",
    "                \n",
    "            elif DATAMODE[\"AONLY\"]:                \n",
    "                output= model(None, None, audio)\n",
    "            \n",
    "            elif DATAMODE[\"RGBDA\"]:                \n",
    "                output= model(imgs, dimgs, audio)\n",
    "                            \n",
    "            loss = F.cross_entropy(output, labels)                \n",
    "                \n",
    "            values, indices = torch.max(torch.softmax(output, dim=1), 1)        \n",
    "            all_preds = np.concatenate((all_preds, indices.cpu().data.numpy()), axis=0)            \n",
    "            \n",
    "            loss.backward()        \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if DEBUG and epoch % 10 == 0:\n",
    "                print(\"Trian preds: \", indices.cpu().data.numpy())\n",
    "\n",
    "        epoch_loss = epoch_loss / len(train_dataset)         \n",
    "        tr_acc = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "\n",
    "        conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "        clf_report = classification_report(all_labels, all_preds)\n",
    "            \n",
    "        if LOG_COMMET:\n",
    "            experiment.log_metric(\"f1_score\", tr_acc, step=epoch)\n",
    "            experiment.log_metric(\"loss\", epoch_loss, step=epoch)        \n",
    "            experiment.log_confusion_matrix(matrix = conf_mat, step=epoch, file_name=\"conf_mat_train_{}.json\".format(epoch))        \n",
    "            experiment.log_text(clf_report, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false,
    "title": "In [13]:"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6cdb4d20c592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b7e0d5572da4>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vivitm3 import ViViTBackbone\n",
    "\n",
    "\n",
    "\n",
    "model = CnnLstm(None, None) # 7:256, 10:256, 14:384\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=param_dict[\"lr\"], weight_decay=param_dict[\"weight_decay\"])\n",
    "\n",
    "for epoch in range(param_dict[\"n_epoch\"]):\n",
    "    print(epoch)\n",
    "    train_epoch(model, train_loader, epoch)\n",
    "    test_epoch(model, val_loader, epoch)    \n",
    "\n",
    "\n",
    "if LOG_COMMET:\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/standardai/conformer/4422c3ea15024224a5d50ead81993c9f\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     test_f1_score [60]  : (0.4833514833514835, 0.5760066833751045)\n",
      "COMET INFO:     test_loss [60]      : (0.07030855040801198, 0.07227720398651927)\n",
      "COMET INFO:     train_f1_score [60] : (0.531109865470852, 0.7501831192247013)\n",
      "COMET INFO:     train_loss [60]     : (0.03361350630543062, 0.043526457622647285)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name : mavi-audio-conformer #09/10/2022 - 15:03:24\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     audio_path        : /home/cak/arda/audio\n",
      "COMET INFO:     audio_path2       : /home/cak/arda/yeni_train/audio\n",
      "COMET INFO:     batch_norm        : True\n",
      "COMET INFO:     batch_size        : 8\n",
      "COMET INFO:     depth_color_path  : /home/cak/arda/depth_imgs_color/\n",
      "COMET INFO:     depth_color_path2 : /home/cak/arda/yeni_train/depth_imgs_color/\n",
      "COMET INFO:     depth_path        : /home/cak/arda/depth_imgs/\n",
      "COMET INFO:     depth_path2       : /home/cak/arda/yeni_train/depth_imgs/\n",
      "COMET INFO:     depth_thresh      : 500\n",
      "COMET INFO:     dropout           : True\n",
      "COMET INFO:     dropout_p         : 0.3\n",
      "COMET INFO:     flip              : True\n",
      "COMET INFO:     img_path          : /home/cak/arda/imgs_awb/\n",
      "COMET INFO:     img_path2         : /home/cak/arda/yeni_train/rgb_imgs/\n",
      "COMET INFO:     img_size          : 224\n",
      "COMET INFO:     j_brightness      : 0.2\n",
      "COMET INFO:     j_contrast        : 0.2\n",
      "COMET INFO:     j_hue             : 0.2\n",
      "COMET INFO:     j_saturation      : 0.2\n",
      "COMET INFO:     jitter            : True\n",
      "COMET INFO:     lr                : 1e-05\n",
      "COMET INFO:     lstm_dropout      : 1\n",
      "COMET INFO:     n_epoch           : 250\n",
      "COMET INFO:     n_layers          : 1\n",
      "COMET INFO:     n_sample_frames   : 8\n",
      "COMET INFO:     n_units           : 256\n",
      "COMET INFO:     normalize         : 1\n",
      "COMET INFO:     num_videos        : 325\n",
      "COMET INFO:     random_crop       : 1\n",
      "COMET INFO:     sampling_mode     : depth\n",
      "COMET INFO:     test_size         : 38\n",
      "COMET INFO:     train_size        : 224\n",
      "COMET INFO:     train_vgg         : 1\n",
      "COMET INFO:     val_size          : 63\n",
      "COMET INFO:     weight_decay      : 1\n",
      "COMET INFO:   Uploads [count]:\n",
      "COMET INFO:     code                   : 1 (21 KB)\n",
      "COMET INFO:     confusion-matrix [120] : 120\n",
      "COMET INFO:     environment details    : 1\n",
      "COMET INFO:     filename               : 1\n",
      "COMET INFO:     installed packages     : 1\n",
      "COMET INFO:     notebook               : 1\n",
      "COMET INFO:     os packages            : 1\n",
      "COMET INFO:     text-sample [120]      : 120\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: Still uploading\n"
     ]
    }
   ],
   "source": [
    "if LOG_COMMET:\n",
    "    experiment.end()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
